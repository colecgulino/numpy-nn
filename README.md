# Neural Network Library in Numpy

To improve on my deep learning fundamentals, I wanted to understand how backpropogation worked for most of the fundamental building blocks of neural networks (i.e. convolution, attention, etc.) Here I will document the results for others who are studying as well.

# Neural Network Layers

[layer.py](docs/layer.md)
â—Š
&rarr; [dense.py](docs/dense.md)
