# Neural Network Library in Numpy

To improve on my deep learning fundamentals, I wanted to understand how backpropogation worked for most of the fundamental building blocks of neural networks (i.e. convolution, attention, etc.) Here I will document the results for others who are studying as well.

# Neural Network Layers

[layer.py](docs/layer.md)

&rarr; [dense.py](docs/dense.md)

[activations.py](docs/activations.md)

[initializers.py](docs/initializers.md)
